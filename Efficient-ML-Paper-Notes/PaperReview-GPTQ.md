# GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers 

## Info: 
___Date submitted____:  31 Oct 2022
___Topic___: #quantization, #transformer, #efficient-ml
___PDFlink___: [[gptq.pdf]] 


## Summary:  

## Result/Achievement:

## Interesting ideas/Takeaway: 

## Personal Note: 



due to the lack of direct hardware support for mixed-precision operands (e.g. FP16 Ã— INT4) on mainstream architectures.